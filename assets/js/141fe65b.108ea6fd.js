"use strict";(self.webpackChunkwhg_training_resources=self.webpackChunkwhg_training_resources||[]).push([[6559],{3905:function(e,t,n){n.d(t,{Zo:function(){return h},kt:function(){return d}});var a=n(7294);function o(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function i(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function r(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?i(Object(n),!0).forEach((function(t){o(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):i(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function l(e,t){if(null==e)return{};var n,a,o=function(e,t){if(null==e)return{};var n,a,o={},i=Object.keys(e);for(a=0;a<i.length;a++)n=i[a],t.indexOf(n)>=0||(o[n]=e[n]);return o}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(a=0;a<i.length;a++)n=i[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(o[n]=e[n])}return o}var s=a.createContext({}),p=function(e){var t=a.useContext(s),n=t;return e&&(n="function"==typeof e?e(t):r(r({},t),e)),n},h=function(e){var t=p(e.components);return a.createElement(s.Provider,{value:t},e.children)},u={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},c=a.forwardRef((function(e,t){var n=e.components,o=e.mdxType,i=e.originalType,s=e.parentName,h=l(e,["components","mdxType","originalType","parentName"]),c=p(n),d=o,m=c["".concat(s,".").concat(d)]||c[d]||u[d]||i;return n?a.createElement(m,r(r({ref:t},h),{},{components:n})):a.createElement(m,r({ref:t},h))}));function d(e,t){var n=arguments,o=t&&t.mdxType;if("string"==typeof e||o){var i=n.length,r=new Array(i);r[0]=c;var l={};for(var s in t)hasOwnProperty.call(t,s)&&(l[s]=t[s]);l.originalType=e,l.mdxType="string"==typeof e?e:o,r[1]=l;for(var p=2;p<i;p++)r[p]=n[p];return a.createElement.apply(null,r)}return a.createElement.apply(null,n)}c.displayName="MDXCreateElement"},4321:function(e,t,n){n.r(t),n.d(t,{assets:function(){return h},contentTitle:function(){return s},default:function(){return d},frontMatter:function(){return l},metadata:function(){return p},toc:function(){return u}});var a=n(7462),o=n(3366),i=(n(7294),n(3905)),r=["components"],l={},s="Model checking the malaria exposure finding",p={unversionedId:"statistical_modelling/Introduction/practicals/malaria_exposure/1000Genomes_genome_wide_comparison",id:"statistical_modelling/Introduction/practicals/malaria_exposure/1000Genomes_genome_wide_comparison",title:"Model checking the malaria exposure finding",description:"Consider the following table from :",source:"@site/docs/statistical_modelling/Introduction/practicals/malaria_exposure/1000Genomes_genome_wide_comparison.md",sourceDirName:"statistical_modelling/Introduction/practicals/malaria_exposure",slug:"/statistical_modelling/Introduction/practicals/malaria_exposure/1000Genomes_genome_wide_comparison",permalink:"/whg-training-resources/statistical_modelling/Introduction/practicals/malaria_exposure/1000Genomes_genome_wide_comparison",draft:!1,editUrl:"https://github.com/whg-training/whg-training-resources/docs/statistical_modelling/Introduction/practicals/malaria_exposure/1000Genomes_genome_wide_comparison.md",tags:[],version:"current",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"simulated_linear_model_practical",permalink:"/whg-training-resources/statistical_modelling/Introduction/practicals/linear_regression/atp2b4/simulated_linear_model_practical"},next:{title:"When asymptotics go wrong",permalink:"/whg-training-resources/statistical_modelling/Introduction/practicals/malaria_exposure/When asymptotics go wrong"}},h={},u=[{value:"Modelling the 2x2 table",id:"modelling-the-2x2-table",level:2},{value:"When asymptotics goes wrong",id:"when-asymptotics-goes-wrong",level:2},{value:"Interpretation",id:"interpretation",level:2}],c={toc:u};function d(e){var t=e.components,n=(0,o.Z)(e,r);return(0,i.kt)("wrapper",(0,a.Z)({},c,n,{components:t,mdxType:"MDXLayout"}),(0,i.kt)("h1",{id:"model-checking-the-malaria-exposure-finding"},"Model checking the malaria exposure finding"),(0,i.kt)("p",null,"Consider the following table from ",(0,i.kt)("a",{parentName:"p",href:"https://science.sciencemag.org/content/348/6235/711"},"https://science.sciencemag.org/content/348/6235/711"),":"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"                                        rs60822373 G allele        rs60822373 C allele\nunexposed populations (Europeans)                      1965        1\nmalaria-exposed populations (Africans).                 707        17\n")),(0,i.kt)("p",null,"It shows counts, taken from the 1000 Genomes Project data (",(0,i.kt)("a",{parentName:"p",href:"https://www.internationalgenome.org"},"https://www.internationalgenome.org"),"), of the reference and non-reference allele of ",(0,i.kt)("inlineCode",{parentName:"p"},"rs60822373")," in two groups of populations: European-ancestry individuals, and individuals from sub-Saharan Africa."),(0,i.kt)("p",null,"The implication of above paper, which was published in the journal Science, is that the rs60822373 'C' allele is likely under positive selection due to malaria.  The table above seems consistent with this, because the main burden of mortality due to malaria occurs in children in sub-Saharan African populations.  If rs60822373 were malaria-protective, individuals carrying it would have a better chance of surviving in these populations, so that the allele might have been pushed to high frequency by natural selection."),(0,i.kt)("p",null,"But how much evidence for natural selection does this table provide?"),(0,i.kt)("p",null,"In this practical we're going to use genome-wide genotype data to ask whether the table above really consistent with evidence for positive natural selection."),(0,i.kt)("p",null,(0,i.kt)("em",{parentName:"p"},"QN.")," Before going further - look at the table and decide what you think about this.  (You can of course apply some statistics here - e.g. a chi-squared test (",(0,i.kt)("inlineCode",{parentName:"p"},"chisq.test()"),") or a Fisher's exact test (",(0,i.kt)("inlineCode",{parentName:"p"},"fisher.test()"),") will give you some sense of the ",(0,i.kt)("em",{parentName:"p"},"statistical")," strength of the data."),(0,i.kt)("h2",{id:"modelling-the-2x2-table"},"Modelling the 2x2 table"),(0,i.kt)("p",null,"Let's load the table into R:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-R"},'exposure.table = matrix(\n    c(\n        1965, 1,\n        707, 17\n    ),\n    ncol = 2,\n    byrow = T,\n    dimnames = list(\n        c( "non-exposed", "exposed" ),\n        c( "G", "C" )\n    )\n)\n')),(0,i.kt)("p",null,"Let's model this table."),(0,i.kt)("p",null,"Individuals in the 1000 Genomes Project were sampled based on their ancestry.  Thus, the ",(0,i.kt)("em",{parentName:"p"},"row sums")," of the above table were essentially determined by the study design and we can treat them as fixed."),(0,i.kt)("p",null,"This means we can model the rows as binomially distributed.  Let \u03d1\u2081 and \u03d1\u2082 be the true allele frequencies of the 'C' allele in nonexposed and exposed populations.  Then we could think of the table as being generated by two binomial distributions:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"\n           G    C        (total)\nunexposed  a    b        (N\u2081 = a+b)\n  exposed  c    d        (N\u2082 = c+d)\n  \n  b ~ binomial( n = N\u2081, p = \u03d1\u2081 )\n  d ~ binomial( n = N\u2082, p = \u03d1\u2082 )\n")),(0,i.kt)("p",null,"This is great because we know how to implement that (using the code we wrote in class):"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-R"},"\n# You need binomial.ll from the stats modelling session first!\n\ntable.ll <- function(\n  data,                                   # This is our table, as a 2x2 matrix\n  params = list( theta = c( 0.5, 0.5 ))   # Theta values for each row.\n) {\n    # We assume binomial sampling in rows\n    # And that the rows are independent\n    # So multiply likelihoods (sum the log-likelihoods) over the rows\n    return (\n        binomial.ll(\n            x = data[1,2],\n            params = list(\n                n = rowSums( data )[1],\n                p = params$theta[1]\n            )\n        )\n        + binomial.ll(\n            x = data[2,2],\n            params = list(\n                n = rowSums( data )[2],\n                p = params$theta[2]\n            )\n        )\n    )\n}\n")),(0,i.kt)("p",null,(0,i.kt)("em",{parentName:"p"},"Note:")," coding-wise we've cheated a bit here - according to our description, really the row sums should form parameters rather than part of the data.  But this way is simpler to write the code. so let's go with it."),(0,i.kt)("p",null,"Let's look at the maximum likelihood model fit:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-R"},"inspect.ll(\n  table.ll,\n  data = exposure.table,\n  params = list( theta = c( 0.5, 0.5 ) )\n)\n")),(0,i.kt)("p",null,"(The rows are independent here so really we are just plotting two binomial log-likelihoods)."),(0,i.kt)("p",null,"It should be immediately obvious that our nice, gaussian, asymptotic behaviour described in the stats modelling session, is not really working for this table.\nEspecially the first row looks pretty skewed.  This is an important point which is described further in [","[when asymptotics go wrong.md]","].  However, there's amore serious problem with this table that we'll get to below."),(0,i.kt)("h1",{id:"a-better-model-via-a-log-odds-ratio"},"A better model via a log odds ratio"),(0,i.kt)("p",null,"A problem with the parameterisation above is that none of the parameters captures what we are really interested in - the difference in frequency between the two rows. "),(0,i.kt)("p",null,"Another problem is that the parameters live in a slightly awkward space - they are in the unit interval ","[0,1]"," but are very close over the boundary.\nThis makes them kind of difficult to work with, both in code and mathematically.  (If you recall, the ",(0,i.kt)("inlineCode",{parentName:"p"},"binomial.ll()")," function we wrote contains special code to make it give sensible answers if you accidentally try a value outside the interval)."),(0,i.kt)("p",null,"A better parameterisation of the table for our problem works by making two changes as follows."),(0,i.kt)("p",null,"First, we replace the frequencies \u03d1\u2081 and \u03d1\u2082 with ",(0,i.kt)("strong",{parentName:"p"},"log odds")," parameters as follows:"),(0,i.kt)("p",null,(0,i.kt)("img",{parentName:"p",src:"https://render.githubusercontent.com/render/math?math=%5Ctextstyle+%5Cgamma_i+%3D+log+%5Cleft%28%5Cfrac%7B%5Ctheta_i%7D%7B1-%5Ctheta_i%7D%5Cright%29",alt:"\\gamma_i = log \\left(\\frac{\\theta_i}{1-\\theta_i}\\right)"})),(0,i.kt)("p",null,"Why the log odds?  "),(0,i.kt)("p",null,"The quintessential way of doing this is to reparameterise in terms of a ",(0,i.kt)("strong",{parentName:"p"},"baseline log odds")," (representing the frequency of C in the first row) and a ",(0,i.kt)("strong",{parentName:"p"},"log odds ratio")," ("),(0,i.kt)("p",null,"To fix that we are going to reparameterise in terms of two other parameters - a ",(0,i.kt)("strong",{parentName:"p"},"baseline")," parameter that"),(0,i.kt)("h2",{id:"when-asymptotics-goes-wrong"},"When asymptotics goes wrong"),(0,i.kt)("p",null,"Look at the top row of the plot.  This picture is typical for binomial likelhoods when the frequency is low or there's not much data - the asymptotic approximation does not work well.  The likelihood becomes skewed, and the normal approximation starts to fail (here overstating the amount of uncertainty on the left, and understating it on the right.)"),(0,i.kt)("p",null,"To show this more clearly - because this is just a binomial, we can also do an exact computation of the estimate and its confidence interval (this function also computes a P-value against the null model that the frequency is zero) using the ",(0,i.kt)("inlineCode",{parentName:"p"},"binom.test()")," function, like this:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-R"},"> binom.test( n = 1966, x = 1, p = 0 )\n\n    Exact binomial test\n\ndata:  1 and 1966\nnumber of successes = 1, number of trials = 1966, p-value < 2.2e-16\nalternative hypothesis: true probability of success is not equal to 0\n95 percent confidence interval:\n 1.287774e-05 2.830707e-03\nsample estimates:\nprobability of success \n           0.000508647 \n\n")),(0,i.kt)("p",null,"The estimate is ",(0,i.kt)("inlineCode",{parentName:"p"},"0.000508647")," (which is 1/1966) and the 95% confidence interval is reported as ",(0,i.kt)("inlineCode",{parentName:"p"},"1.287774e-05")," to ",(0,i.kt)("inlineCode",{parentName:"p"},"2.830707e-03"),".  On the other hand the asymptotic method above computes the standard error as ",(0,i.kt)("inlineCode",{parentName:"p"},"0.0005093123")," (see the function output) so we can compute asymptotic estimated confidence intervals as:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-R"},'theta = 1/1966\nse = 0.0005093123\nsprintf( "95%% CI = %.4f (%.4f - %.4f)", theta, theta - 1.96 * se, theta + 1.96 * se )\n')),(0,i.kt)("p",null,"This reports ",(0,i.kt)("inlineCode",{parentName:"p"},"95% CI = 0.0005 (-0.0005 - 0.0015)"),".  As the plot indicates, this asymptotic confidence interval overstates uncertainty on the left but understates it on the right.  Similarly, an asymptotic computation of a P-value (for example the Wald test P-value, which just looks at how far the estimate is in the tails of the approximating normal distribution) gets it somewhat wrong - here is a ",(0,i.kt)("strong",{parentName:"p"},"one-sided test")," against the null that \u03d1\u2081 = 0:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"# compute Wald test p-value using cumulative distribution function of Gaussian:\npnorm( theta, mean = 0, sd = 0.0005093123, lower.tail = F ) \n")),(0,i.kt)("p",null,"This reports the P-value as ",(0,i.kt)("inlineCode",{parentName:"p"},"0.15"),".  But, since we've observed one C allele, it is ",(0,i.kt)("em",{parentName:"p"},"completely impossible")," that the frequency is zero!  The true p-value is actually zero, because \u03d1\u2081 = 0 ",(0,i.kt)("em",{parentName:"p"},"never")," generates any C alleles at all."),(0,i.kt)("p",null,"On the other hand, when there's lots of data and the frequency is not close to zero (e.g. the second row of the plot) things are fine.  The likelihood becomes a fairly well-rounded citizen (literally so - it becomes approximately normal, i.e. its log has approximately constant curvature)."),(0,i.kt)("h2",{id:""}),(0,i.kt)("p",null,"We can compare this to our fit by converting back to standard errors like this:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-R"},"  theta1 = 0.000508647 \n  ci = c( 1.287774e-05, 2.830707e-03 )\n  se = theta1 - ci\n")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-R"},"")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"\nThus the rows\n\n\n\nEssentially this means the row sums in the above table were determined beforehand.  A reasonable model for the data is therefore *that the rows are binomially distributed*.  If \u03b8 stands for\n\n")),(0,i.kt)("p",null,"row1",(0,i.kt)("br",{parentName:"p"}),"\n","row2 "),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"\nWe can therefore model the tbale like so:\n```R\n")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-R"},"> theTable\n               G  C\nnon-exposed 1965  1\nexposed      707 17\n")),(0,i.kt)("p",null,"The allele frequencies in the top and bottom row are very different - the C allele has almost zero frequency in the top row, but around\nThe odds ratio computed from this table is 47 - implying the frequencies of this variant are very different in European and African populations.\nThe Egan et al paper suggests this is evidence for malaria-driven natural selection of this allele."),(0,i.kt)("p",null,"How could we test that?  Well one way is "),(0,i.kt)("p",null,"And a test of this table ( ",(0,i.kt)("inlineCode",{parentName:"p"},"chisq.test()")," or ",(0,i.kt)("inlineCode",{parentName:"p"},"fisher.test()")," shows that this is highly statistically significant.  That means:"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("em",{parentName:"li"},"If")," the data were truly sampled from a binomial distribution with given frequency in each row."),(0,i.kt)("li",{parentName:"ul"},"and ",(0,i.kt)("em",{parentName:"li"},"if")," the true odds ratio was zero."),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("em",{parentName:"li"},"then")," a table with such a large odds ratio would almost never occur.")),(0,i.kt)("p",null,"(",(0,i.kt)("em",{parentName:"p"},"NB.")," The assumptions of Fisher's exact test and of the chi-squared test, which corresponds to 'binomial sampling in rows', are slightly different.\nFisher's exact test is actually conditional on both row and column sums, which reduces things to one parameter.  This doesn't matter much for most tables in practice because the row and column sums are not usually very informative about the odds ratio itself.)"),(0,i.kt)("p",null,"Does this table provide evidence that the malaria-exposed population allele frequency is higher because of natural selection due to malaria?  (That's the implicit message of this part of the original paper: ","[https://science.sciencemag.org/content/348/6235/711]","(Egan et al Science 2015) )."),(0,i.kt)("p",null,"One of the advantages of genome-wide analysis is that we have a lot of data to work with - and this can be used for model checking. Suppose\nwe compare our finding with all other 'similar' variants in the genome. They have all been\ngenotyped on the same set of samples, and as such they represent the same sampling biases (if any) and the same demography. However, due to recombination they also represent many independent draws from the genealogical history of the sample."),(0,i.kt)("p",null,"The file ",(0,i.kt)("inlineCode",{parentName:"p"},"allele_counts_with_exposed_count\\=17.csv.gz")," contains data for all the alleles in the 1000 Genomes populations studied, that have the same allele count of 17 in the exposed populations. Load it:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},'X = read.csv(\n    "allele_counts_with_exposed_count=17.csv.gz",\n    header = T,      # Tell R our file has a row of column names\n    as.is = T,       # Ask R to please not transmogrify any data\n    check.names = F  # Also ask R not to mess around with column names \n)\n\n# Take a look\nhead(X)\n')),(0,i.kt)("p",null,"Let's first sanity check I've computed the data right."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"range( X$`exposed:B` )\n")),(0,i.kt)("p",null,"It's often best to first collect numerical data into a matrix - this is a container of numbers that is layed out nicely in memory for computation.  (Unlike a data frame which holds columns of data of different types).  The ",(0,i.kt)("inlineCode",{parentName:"p"},"as.matrix()")," function does this:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},'data = as.matrix( X[, c( "unexposed:A", "unexposed:B", "exposed:A", "exposed:B" )])\n# Take a look\nhead( data )\n')),(0,i.kt)("p",null,"Let's compute the frequency of each variant:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"X$frequency = (data[,2] + data[,4]) / rowSums(data)\n\n# plot it\nhist(\n    X$frequency,\n    breaks = 50, #\xa0how many bars across the plot?\n    xlab = \"Allele frequency\"\n)\nabline( v = X$frequency[ which( X$rsid == 'rs60822373' )], col = 'red' )\n")),(0,i.kt)("p",null,"In this dataset we have conditioned on the exposed population frequency, so it's proabably more sensible to look at the frequency in non-exposed populations:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"hist(\n    X$`unexposed:B`,\n    breaks = 50,\n    xlab = \"log Odds ratio\",\n    main = \"Alleles with count = 17 in malaria-exposed populations\"\n)\n\n# The value for our SNP is 1, so let's replot clamped to 50 to focus better on that area\nhist(\n    pmin( X$`unexposed:B`, 50 ),\n    breaks = 50,\n    xlab = \"Count of allele in non-exposed pops\",\n    main = \"Alleles with count = 17 in malaria-exposed populations\",\n    #xlim = c( -0.5, 0.5 ),\n    #xaxt = 'n'  # Turn off the axis so we can draw our own\n)\n# Let's put a line where rs60822373 is:\nabline(\n    v = X$`unexposed:B`[ which( X$rsid == 'rs60822373' )],\n    col = 'red'\n)\ntext(\n    x = X$`unexposed:B`[ which( X$rsid == 'rs60822373' )],\n    y = 50000,\n    adj = 0,\n    pos = 4, # draw text to right\n    label = \"rs60822373\",\n    col = 'red'\n)\n")),(0,i.kt)("p",null,"Sure enough there are a bunch of alleles at higher counts in non-exposed populations.  But there is also a big spike at zero. "),(0,i.kt)("p",null,"We could also mesaure differences directly using the log-odds ratio:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"X$logOR = log( ((data[,1]) * (data[,4])) / ( (data[,2]) * (data[,3])) )\n")),(0,i.kt)("p",null,"Unfortunately if you do this you'll find many are infinite (due to a 0 in the table):"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"length( which( X$logOR == Inf ))\n")),(0,i.kt)("p",null,"We will therefore compute the log OR by adding a dummy count of  1 to each cell.  This amounts to adding prior information.  This amounts to adding a prior.  (If you want to see what that prior looks like, plot the ",(0,i.kt)("inlineCode",{parentName:"p"},"table.ll.reparameterised()")," function on a table full of 1's.)"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"X$logOR = log( ((data[,1]+1) * (data[,4]+1)) / ( (data[,2]+1) * (data[,3]+1)) )\n")),(0,i.kt)("p",null,"Let's look at the distribution of the logOR:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"# Compute log OR with additional prior count of 1 in each cell to avoid infinities.\n\nhist(\n    X$logOR,\n    breaks = 50,\n    xlab = \"log Odds ratio\",\n    main = \"Alleles with count = 17 in malaria-exposed populations\"\n)\nabline(\n    v = X$logOR[ which( X$rsid == 'rs60822373' )],\n    col = 'red'\n)\n")),(0,i.kt)("p",null,"(Because the 3rd and 4th cells of our table are constant here, this has exactly the same information as above but expressed as a log OR)."),(0,i.kt)("p",null,"We originally computed a P-value which indicated that the table was very unlikely to arise by chance under the null.  But now we find that many other variants have at least as extreme counts! We can use our data to get an empirical P-value:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"empirical.P = length( which( X$`unexposed:B` <= 1 )) / nrow(X)\n")),(0,i.kt)("p",null,"The answer is ",(0,i.kt)("inlineCode",{parentName:"p"},"0.49"),".  So about half the SNPs in the genome with this low count in malaria-exposed populations, have equally low frequencies in non-exposed populations.  They can't all be under selection can they?"),(0,i.kt)("p",null,'The null model says that all variation in the effect size (the log OR) is due to sampling variation true log OR = 0).  It should be distributed as a N(0, SE^2).  (all the standard errors will be similar here because of the choice of variant).  To test how well the null model fits, we can plot standard-error-adjusted effect sizes ("Z-scores") against a gaussian distribution.  First compute the standard error:'),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"# for table\n#   a b\n#   c d\n# the formula is sqrt( 1/a + 1/b + 1/c + 1/d )\nX$SE = sqrt( rowSums( 1/(data+1) ))\n")),(0,i.kt)("p",null,(0,i.kt)("em",{parentName:"p"},"Warning"),": this standard error is not quite correct due to our use of a prior additional count of 1 to each cell, which is fixed and not sampled.  This won't matter for this exercise."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"region = c( -10, 10 )\nhist(\n    X$logOR / X$SE,\n    breaks = 100,\n    xlab = \"x score\",\n    xlim = region,\n    ylim = c( 0, 0.5 ),\n    main = \"Alleles matching O blood group frequency\",\n    freq = FALSE #\xa0plot y axis values as a density, for comparison\n)\nx = seq( from = region[1], to = region[2], by = 0.01 )\npoints(\n    x,\n    dnorm( x, mean = 0, sd = 1 ),\n    type = 'l',\n    col = 'red'\n)\n")),(0,i.kt)("p",null,"The null model, at least as expressed through the log OR and its standard error, is not even close to being true."),(0,i.kt)("p",null,"For completeness / comparison, a more sensitive plot that we don't really need here, is a quantile-quantile plot, which plots ordered statistics against their expection.  We can do that here:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"zscores = sort( X$logOR / X$SE )\nexpected = qnorm( (1:length(zscores))/(length(zscores)+1)) # Normal distribution quantiles\nplot(\n    expected,\n    zscores,\n    pch = 19 # nice round dots\n)\nabline( a = 0, b = 1, col = 'red' )\ngrid()\n")),(0,i.kt)("p",null,"If the expected and true distributions were similar the points would lie near the line."),(0,i.kt)("h2",{id:"interpretation"},"Interpretation"),(0,i.kt)("p",null,"Even though this table and the one for O blood group looked superficially similar, they differ in a couple of ways.  "),(0,i.kt)("p",null,"One is that the small counts in some cells of this table mean we are at the boundary of where traditional statistics based on asymptotic assumptions holds.  This was the problem with the infinite log ORs and means we should be careful about just rolling out standard tools.  (The original authors used 'Fishers Exact Test' which does deal with this issue, although it introduces additional assumptions.)"),(0,i.kt)("p",null,"Another more serious issue is that the assumed null model - that frequencies should be similar between the two groups - is not even close to being correct for most variants in the genome.  This is a serious issue which should cause us to reevaluate our assumptions."))}d.isMDXComponent=!0}}]);