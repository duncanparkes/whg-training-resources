"use strict";(self.webpackChunkwhg_training_resources=self.webpackChunkwhg_training_resources||[]).push([[1807],{3905:function(e,t,a){a.d(t,{Zo:function(){return p},kt:function(){return m}});var n=a(7294);function r(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function i(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,n)}return a}function o(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?i(Object(a),!0).forEach((function(t){r(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):i(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function l(e,t){if(null==e)return{};var a,n,r=function(e,t){if(null==e)return{};var a,n,r={},i=Object.keys(e);for(n=0;n<i.length;n++)a=i[n],t.indexOf(a)>=0||(r[a]=e[a]);return r}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(n=0;n<i.length;n++)a=i[n],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(r[a]=e[a])}return r}var s=n.createContext({}),d=function(e){var t=n.useContext(s),a=t;return e&&(a="function"==typeof e?e(t):o(o({},t),e)),a},p=function(e){var t=d(e.components);return n.createElement(s.Provider,{value:t},e.children)},c={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},u=n.forwardRef((function(e,t){var a=e.components,r=e.mdxType,i=e.originalType,s=e.parentName,p=l(e,["components","mdxType","originalType","parentName"]),u=d(a),m=r,f=u["".concat(s,".").concat(m)]||u[m]||c[m]||i;return a?n.createElement(f,o(o({ref:t},p),{},{components:a})):n.createElement(f,o({ref:t},p))}));function m(e,t){var a=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var i=a.length,o=new Array(i);o[0]=u;var l={};for(var s in t)hasOwnProperty.call(t,s)&&(l[s]=t[s]);l.originalType=e,l.mdxType="string"==typeof e?e:r,o[1]=l;for(var d=2;d<i;d++)o[d]=a[d];return n.createElement.apply(null,o)}return n.createElement.apply(null,a)}u.displayName="MDXCreateElement"},1350:function(e,t,a){a.r(t),a.d(t,{assets:function(){return p},contentTitle:function(){return s},default:function(){return m},frontMatter:function(){return l},metadata:function(){return d},toc:function(){return c}});var n=a(7462),r=a(3366),i=(a(7294),a(3905)),o=["components"],l={},s=void 0,d={unversionedId:"statistical_modelling/Introduction/practicals/linear_regression/atp2b4/simulated_linear_model_practical",id:"statistical_modelling/Introduction/practicals/linear_regression/atp2b4/simulated_linear_model_practical",title:"simulated_linear_model_practical",description:"Linear regression for simulated data",source:"@site/docs/statistical_modelling/Introduction/practicals/linear_regression/atp2b4/simulated_linear_model_practical.md",sourceDirName:"statistical_modelling/Introduction/practicals/linear_regression/atp2b4",slug:"/statistical_modelling/Introduction/practicals/linear_regression/atp2b4/simulated_linear_model_practical",permalink:"/whg-training-resources/statistical_modelling/Introduction/practicals/linear_regression/atp2b4/simulated_linear_model_practical",draft:!1,editUrl:"https://github.com/whg-training/whg-training-resources/docs/statistical_modelling/Introduction/practicals/linear_regression/atp2b4/simulated_linear_model_practical.md",tags:[],version:"current",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"atp2b4_practical",permalink:"/whg-training-resources/statistical_modelling/Introduction/practicals/linear_regression/atp2b4/atp2b4_practical"},next:{title:"Model checking the malaria exposure finding",permalink:"/whg-training-resources/statistical_modelling/Introduction/practicals/malaria_exposure/1000Genomes_genome_wide_comparison"}},p={},c=[{value:"Linear regression for simulated data",id:"linear-regression-for-simulated-data",level:2},{value:"simulating some data",id:"simulating-some-data",level:3},{value:"What is linear regression?",id:"what-is-linear-regression",level:3},{value:"fitting the regression",id:"fitting-the-regression",level:3},{value:"What was the likelihood?",id:"what-was-the-likelihood",level:3},{value:"Regression diagnostics",id:"regression-diagnostics",level:3},{value:"Fitting to real data",id:"fitting-to-real-data",level:2}],u={toc:c};function m(e){var t=e.components,l=(0,r.Z)(e,o);return(0,i.kt)("wrapper",(0,n.Z)({},u,l,{components:t,mdxType:"MDXLayout"}),(0,i.kt)("h2",{id:"linear-regression-for-simulated-data"},"Linear regression for simulated data"),(0,i.kt)("h3",{id:"simulating-some-data"},"simulating some data"),(0,i.kt)("p",null,"One of the simplest ways to explore statistical models is by simulating.  Let's simulate some data from a simple linear regression and then fit it."),(0,i.kt)("p",null,"First let's make a gaussian predictor variable:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"  N = 100\n  X = rnorm( N )\n")),(0,i.kt)("p",null,"Now let's make a gaussian outcome variable that is correlated with X. To do this, we'll simply take some multiple of X and add some noise:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"Y = 0.5*X + rnorm( N, sd = sqrt(0.75) )\n")),(0,i.kt)("p",null,"(I chose that standard deviation because of way variance scales.  If ",(0,i.kt)("em",{parentName:"p"},"X")," has variance 1, then 0.5 \xd7 ",(0,i.kt)("em",{parentName:"p"},"X")," has variance 0.25.  Therefore we need to add a variable with variance 0.75 to get back to variance 1.  And of course the standard deviation is the square root of the variance.)"),(0,i.kt)("p",null,"Let's see what that looks like"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},'library( tidyverse )\ndata = tibble( X = X, Y = Y )\nplot( data$X, data$Y, xlab = "X", ylab = "Y", pch = 19 )\n')),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"Note.")," I am using the ",(0,i.kt)("a",{parentName:"p",href:"https://www.tidyverse.org"},"tidyverse")," for these examples. If you can't install it (or don't\nwant to), you can use ",(0,i.kt)("inlineCode",{parentName:"p"},"data.frame()")," instead of ",(0,i.kt)("inlineCode",{parentName:"p"},"tibble()")," above."),(0,i.kt)("h3",{id:"what-is-linear-regression"},"What is linear regression?"),(0,i.kt)("p",null,"Linear regression models the outcome ",(0,i.kt)("em",{parentName:"p"},"Y")," in terms of a ",(0,i.kt)("strong",{parentName:"p"},"linear effect of the predictor")," ",(0,i.kt)("em",{parentName:"p"},"X"),".  In the simplest form with one predictor variable, it estimates the slope of the best-fitting line.  See the ",(0,i.kt)("a",{target:"_blank",href:a(2244).Z},"linear regression outline")," for a diagram."),(0,i.kt)("h3",{id:"fitting-the-regression"},"fitting the regression"),(0,i.kt)("p",null,"The function ",(0,i.kt)("inlineCode",{parentName:"p"},"lm()")," in R fits a linear regression model.  The syntax is:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"lm( [outcome] ~ [predictor], data = [data frame] )\n")),(0,i.kt)("p",null,"So let's fit our data:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"fit = lm( Y ~ X, data = data )\n")),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"Note.")," This fits the linear regression likelihood, finding its maximum.  It does not directly support including a prior distribution.  We'll work with that for now."),(0,i.kt)("p",null,"If you run this you'll see something like: "),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"> lm( Y ~ X, data = data )\n\nCall:\nlm(formula = Y ~ X, data = data)\n\nCoefficients:\n(Intercept)            X  \n   -0.09646      0.54167  \n")),(0,i.kt)("p",null,"A better summary is provided by (guess what?) ",(0,i.kt)("inlineCode",{parentName:"p"},"summary()"),":"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"fit = lm( Y ~ X, data = data `\nsummary( fit )\n")),(0,i.kt)("p",null,"In particular it's useful to grab the ",(0,i.kt)("strong",{parentName:"p"},"coefficients")," (i.e. the maximum likelihood parameter values):"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"coeffs = summary( fit )$coeff\nprint( coeffs[,1:2])\n")),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"Question"),". What do these coefficients mean?  "),(0,i.kt)("p",null,"Let's plot them:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},'# plot the data:\nplot(\n  data$X,\n  data$Y,\n  pch = 19,\n  xlab = "X",\n  ylab = "Y"\n)\n\nabline( coef = coeffs[,1], col = \'red\', lwd = 2 )\n')),(0,i.kt)("p",null,"That's just the best-fitting line... let's also plot 95% upper and lower estimates of the slope:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"abline( coef = coeffs[,1] + c( 0, coeffs[2,'Std. Error'] * 1.96 ), col = 'red', lty = 2 )\nabline( coef = coeffs[,1] - c( 0, coeffs[2,'Std. Error'] * 1.96 ), col = 'red', lty = 2 )\n")),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"Note.")," The value 1.96 used above comes from the following.  If you work out the area of 95% mass under a standard Gaussian distribution:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"qnorm( c( 0.025, 0.975 ))\n")),(0,i.kt)("p",null,"you will see it stretches from about -1.96 to 1.96.  That is, a variable that is Gaussian distributed has 95% of its mass within +/- 1.96 standard deviations of the mean."),(0,i.kt)("p",null,"In the simulation above, the true effect size was 0.5 and the intercept (i.e. the mean of Y) was zero.  Let's plot that as well:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"abline( coef = c(0,0.5), col = 'grey', lty = 2 )\n")),(0,i.kt)("p",null,"if your simulation is like mine... the fit is 'not bad'."),(0,i.kt)("h3",{id:"what-was-the-likelihood"},"What was the likelihood?"),(0,i.kt)("p",null,"The logarithm of the likelihood can be obtained with the ",(0,i.kt)("inlineCode",{parentName:"p"},"logLik()")," function:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"logLik(fit)\n")),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"Note.")," Although you can exponentiate to get the (non-logged) likelihood, it is very very small. Working in log space is typically better."),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"Question.")," suppose you go back and simulate again with twice as many data points.  How would the (log)likelihood change?  Would it get bigger or smaller?  Why?"),(0,i.kt)("h3",{id:"regression-diagnostics"},"Regression diagnostics"),(0,i.kt)("p",null,"The linear regression model says that the points should be distributed around the line with the same error distribution - a Gaussian with mean 0 and fixed variance.  Let's make a plot to test that."),(0,i.kt)("p",null,"First, the fitted residual standard deviation can be obtained with the ",(0,i.kt)("inlineCode",{parentName:"p"},"sigma")," function:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"residual.sd = sigma(fit)\n")),(0,i.kt)("p",null,"Now let's generate N quantiles of a Gaussian with that standard deviation - these are what we ",(0,i.kt)("em",{parentName:"p"},"expect")," the data to look like if the model is good:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"# I use 1:N/(N+1) to get N equally spaced quantiles\nexpected.residuals = qnorm( 1:N/(N+1), sd = residual.sd )\n# You could also use sort( rnorm( N, sd = residual.ld )) here\n")),(0,i.kt)("p",null,"Now let's plot a ",(0,i.kt)("strong",{parentName:"p"},"qq-plot")," comparing the expected to the observed residuals:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},'# two ways to get residuals - calculate directly:\nobserved.residuals = sort( data$Y - predict(fit) )\n# or just get them from the fit object:\nobserved.residuals = sort( fit$residuals )\n\nplot(\n  expected.residuals,\n  observed.residuals,\n  pch = 19,\n  xlab = "Expected residuals",\n  ylab = "Observed residuals"\n)\nabline( a = 0, b = 1, col = \'red\' )\n')),(0,i.kt)("p",null,"They look... pretty close to the line?"),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"note.")," Incidentally it is useful to have a function to do this.  How about:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},'lm.qqplot = function( fit ) {\n  N = nrow( fit$model )\n  residual.sd = sigma(fit)\n  expected.residuals = qnorm( 1:N/(N+1), sd = residual.sd )\n  observed.residuals = sort( fit$residuals )\n  plot(\n    expected.residuals,\n    observed.residuals,\n    pch = 19,\n    xlab = "Expected residuals",\n    ylab = "Observed residuals"\n  )\n  abline( a = 0, b = 1, col = \'red\' )\n}\nlm.qqplot(fit)\n')),(0,i.kt)("h2",{id:"fitting-to-real-data"},"Fitting to real data"),(0,i.kt)("p",null,"Now go to the ",(0,i.kt)("a",{parentName:"p",href:"/whg-training-resources/statistical_modelling/Introduction/practicals/linear_regression/atp2b4/atp2b4_practical"},"ATP2B4 expression practical"),"."))}m.isMDXComponent=!0},2244:function(e,t,a){t.Z=a.p+"assets/files/Linear regression-4da296b307c32abffb6a2eaae3cdcd00.pdf"}}]);