"use strict";(self.webpackChunkwhg_training_resources=self.webpackChunkwhg_training_resources||[]).push([[1401],{3905:function(e,t,n){n.d(t,{Zo:function(){return p},kt:function(){return h}});var o=n(7294);function a(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function r(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);t&&(o=o.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,o)}return n}function i(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?r(Object(n),!0).forEach((function(t){a(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):r(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function s(e,t){if(null==e)return{};var n,o,a=function(e,t){if(null==e)return{};var n,o,a={},r=Object.keys(e);for(o=0;o<r.length;o++)n=r[o],t.indexOf(n)>=0||(a[n]=e[n]);return a}(e,t);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(o=0;o<r.length;o++)n=r[o],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(a[n]=e[n])}return a}var l=o.createContext({}),c=function(e){var t=o.useContext(l),n=t;return e&&(n="function"==typeof e?e(t):i(i({},t),e)),n},p=function(e){var t=c(e.components);return o.createElement(l.Provider,{value:t},e.children)},u={inlineCode:"code",wrapper:function(e){var t=e.children;return o.createElement(o.Fragment,{},t)}},m=o.forwardRef((function(e,t){var n=e.components,a=e.mdxType,r=e.originalType,l=e.parentName,p=s(e,["components","mdxType","originalType","parentName"]),m=c(n),h=a,f=m["".concat(l,".").concat(h)]||m[h]||u[h]||r;return n?o.createElement(f,i(i({ref:t},p),{},{components:n})):o.createElement(f,i({ref:t},p))}));function h(e,t){var n=arguments,a=t&&t.mdxType;if("string"==typeof e||a){var r=n.length,i=new Array(r);i[0]=m;var s={};for(var l in t)hasOwnProperty.call(t,l)&&(s[l]=t[l]);s.originalType=e,s.mdxType="string"==typeof e?e:a,i[1]=s;for(var c=2;c<r;c++)i[c]=n[c];return o.createElement.apply(null,i)}return o.createElement.apply(null,n)}m.displayName="MDXCreateElement"},8213:function(e,t,n){n.r(t),n.d(t,{assets:function(){return p},contentTitle:function(){return l},default:function(){return h},frontMatter:function(){return s},metadata:function(){return c},toc:function(){return u}});var o=n(7462),a=n(3366),r=(n(7294),n(3905)),i=["components"],s={},l="When asymptotics go wrong",c={unversionedId:"statistical_modelling/Introduction/practicals/a_tale_of_two_2x2_tables/When asymptotics go wrong",id:"statistical_modelling/Introduction/practicals/a_tale_of_two_2x2_tables/When asymptotics go wrong",title:"When asymptotics go wrong",description:"If you've been following the practical so far, you'll have a plot of the parameter estimates from a binomial likelihood for both rows of our table.",source:"@site/docs/statistical_modelling/Introduction/practicals/a_tale_of_two_2x2_tables/When asymptotics go wrong.md",sourceDirName:"statistical_modelling/Introduction/practicals/a_tale_of_two_2x2_tables",slug:"/statistical_modelling/Introduction/practicals/a_tale_of_two_2x2_tables/When asymptotics go wrong",permalink:"/whg-training-resources/statistical_modelling/Introduction/practicals/a_tale_of_two_2x2_tables/When asymptotics go wrong",draft:!1,editUrl:"https://github.com/whg-training/whg-training-resources/docs/statistical_modelling/Introduction/practicals/a_tale_of_two_2x2_tables/When asymptotics go wrong.md",tags:[],version:"current",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"O_bld_group_genome_wide_comparison",permalink:"/whg-training-resources/statistical_modelling/Introduction/practicals/a_tale_of_two_2x2_tables/O_bld_group_genome_wide_comparison"},next:{title:"A tale of two 2x2 tables",permalink:"/whg-training-resources/statistical_modelling/Introduction/practicals/a_tale_of_two_2x2_tables/introduction"}},p={},u=[],m={toc:u};function h(e){var t=e.components,n=(0,a.Z)(e,i);return(0,r.kt)("wrapper",(0,o.Z)({},m,n,{components:t,mdxType:"MDXLayout"}),(0,r.kt)("h1",{id:"when-asymptotics-go-wrong"},"When asymptotics go wrong"),(0,r.kt)("p",null,"If you've been following the practical so far, you'll have a plot of the parameter estimates from a binomial likelihood for both rows of our table."),(0,r.kt)("p",null,"Look at the top row of the plot.  This picture is typical for binomial likelhoods when the frequency is low or there's not much data - the asymptotic approximation does not work well.  The likelihood becomes skewed, and the normal approximation starts to fail (here overstating the amount of uncertainty on the left, and understating it on the right.)"),(0,r.kt)("p",null,"To show this more clearly - because this is just a binomial, we can also do an exact computation of the estimate and its confidence interval (this function also computes a P-value against the null model that the frequency is zero) using the ",(0,r.kt)("inlineCode",{parentName:"p"},"binom.test()")," function, like this:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-R"},"> binom.test( n = 1966, x = 1, p = 0 )\n\n    Exact binomial test\n\ndata:  1 and 1966\nnumber of successes = 1, number of trials = 1966, p-value < 2.2e-16\nalternative hypothesis: true probability of success is not equal to 0\n95 percent confidence interval:\n 1.287774e-05 2.830707e-03\nsample estimates:\nprobability of success \n           0.000508647 \n\n")),(0,r.kt)("p",null,"The estimate is ",(0,r.kt)("inlineCode",{parentName:"p"},"0.000508647")," (which is 1/1966) and the 95% confidence interval is reported as ",(0,r.kt)("inlineCode",{parentName:"p"},"1.287774e-05")," to ",(0,r.kt)("inlineCode",{parentName:"p"},"2.830707e-03"),".  On the other hand the asymptotic method above computes the standard error as ",(0,r.kt)("inlineCode",{parentName:"p"},"0.0005093123")," (see the function output) so we can compute asymptotic estimated confidence intervals as:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-R"},'theta = 1/1966\nse = 0.0005093123\nsprintf( "95%% CI = %.4f (%.4f - %.4f)", theta, theta - 1.96 * se, theta + 1.96 * se )\n')),(0,r.kt)("p",null,"This reports ",(0,r.kt)("inlineCode",{parentName:"p"},"95% CI = 0.0005 (-0.0005 - 0.0015)"),".  As the plot indicates, this asymptotic confidence interval overstates uncertainty on the left but understates it on the right.  Similarly, an asymptotic computation of a P-value (for example the Wald test P-value, which just looks at how far the estimate is in the tails of the approximating normal distribution) gets it somewhat wrong - here is a ",(0,r.kt)("strong",{parentName:"p"},"one-sided test")," against the null that \u03d1\u2081 = 0:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"# compute Wald test p-value using cumulative distribution function of Gaussian:\npnorm( theta, mean = 0, sd = 0.0005093123, lower.tail = F ) \n")),(0,r.kt)("p",null,"This reports the P-value as ",(0,r.kt)("inlineCode",{parentName:"p"},"0.15"),".  But, since we've observed one C allele, it is ",(0,r.kt)("em",{parentName:"p"},"completely impossible")," that the frequency is zero!  The true p-value is actually zero, because \u03d1\u2081 = 0 ",(0,r.kt)("em",{parentName:"p"},"never")," generates any C alleles at all."),(0,r.kt)("p",null,"On the other hand, when there's lots of data and the frequency is not close to zero (e.g. the second row of the plot) things are fine.  The likelihood becomes a fairly well-rounded citizen (literally so - it becomes approximately normal, i.e. its log has approximately constant curvature)."))}h.isMDXComponent=!0}}]);