"use strict";(self.webpackChunkwhg_training_resources=self.webpackChunkwhg_training_resources||[]).push([[783],{3905:function(e,n,t){t.d(n,{Zo:function(){return l},kt:function(){return g}});var a=t(7294);function r(e,n,t){return n in e?Object.defineProperty(e,n,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[n]=t,e}function i(e,n){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);n&&(a=a.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),t.push.apply(t,a)}return t}function o(e){for(var n=1;n<arguments.length;n++){var t=null!=arguments[n]?arguments[n]:{};n%2?i(Object(t),!0).forEach((function(n){r(e,n,t[n])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):i(Object(t)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(t,n))}))}return e}function s(e,n){if(null==e)return{};var t,a,r=function(e,n){if(null==e)return{};var t,a,r={},i=Object.keys(e);for(a=0;a<i.length;a++)t=i[a],n.indexOf(t)>=0||(r[t]=e[t]);return r}(e,n);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(a=0;a<i.length;a++)t=i[a],n.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(r[t]=e[t])}return r}var c=a.createContext({}),u=function(e){var n=a.useContext(c),t=n;return e&&(t="function"==typeof e?e(n):o(o({},n),e)),t},l=function(e){var n=u(e.components);return a.createElement(c.Provider,{value:n},e.children)},p={inlineCode:"code",wrapper:function(e){var n=e.children;return a.createElement(a.Fragment,{},n)}},d=a.forwardRef((function(e,n){var t=e.components,r=e.mdxType,i=e.originalType,c=e.parentName,l=s(e,["components","mdxType","originalType","parentName"]),d=u(t),g=r,m=d["".concat(c,".").concat(g)]||d[g]||p[g]||i;return t?a.createElement(m,o(o({ref:n},l),{},{components:t})):a.createElement(m,o({ref:n},l))}));function g(e,n){var t=arguments,r=n&&n.mdxType;if("string"==typeof e||r){var i=t.length,o=new Array(i);o[0]=d;var s={};for(var c in n)hasOwnProperty.call(n,c)&&(s[c]=n[c]);s.originalType=e,s.mdxType="string"==typeof e?e:r,o[1]=s;for(var u=2;u<i;u++)o[u]=t[u];return a.createElement.apply(null,o)}return a.createElement.apply(null,t)}d.displayName="MDXCreateElement"},9733:function(e,n,t){t.r(n),t.d(n,{assets:function(){return l},contentTitle:function(){return c},default:function(){return g},frontMatter:function(){return s},metadata:function(){return u},toc:function(){return p}});var a=t(7462),r=t(3366),i=(t(7294),t(3905)),o=["components"],s={sidebar_position:20},c="Appendix - understanding sequence duplication levels",u={unversionedId:"next_generation_sequencing/Introduction to next-generation sequencing data analysis/duplicates",id:"next_generation_sequencing/Introduction to next-generation sequencing data analysis/duplicates",title:"Appendix - understanding sequence duplication levels",description:"Duplicates arise naturally in sequencing from random fragments that just happen to have",source:"@site/docs/next_generation_sequencing/Introduction to next-generation sequencing data analysis/duplicates.md",sourceDirName:"next_generation_sequencing/Introduction to next-generation sequencing data analysis",slug:"/next_generation_sequencing/Introduction to next-generation sequencing data analysis/duplicates",permalink:"/whg-training-resources/next_generation_sequencing/Introduction to next-generation sequencing data analysis/duplicates",draft:!1,editUrl:"https://github.com/whg-training/whg-training-resources/docs/next_generation_sequencing/Introduction to next-generation sequencing data analysis/duplicates.md",tags:[],version:"current",sidebarPosition:20,frontMatter:{sidebar_position:20},sidebar:"tutorialSidebar",previous:{title:"Appendix - how accurate is Illumina sequencing?",permalink:"/whg-training-resources/next_generation_sequencing/Introduction to next-generation sequencing data analysis/De_novo_error_rate_estimation"},next:{title:"Appendix - read trimming",permalink:"/whg-training-resources/next_generation_sequencing/Introduction to next-generation sequencing data analysis/Read_trimming"}},l={},p=[],d={toc:p};function g(e){var n=e.components,s=(0,r.Z)(e,o);return(0,i.kt)("wrapper",(0,a.Z)({},d,s,{components:n,mdxType:"MDXLayout"}),(0,i.kt)("h1",{id:"appendix---understanding-sequence-duplication-levels"},"Appendix - understanding sequence duplication levels"),(0,i.kt)("p",null,"Duplicates arise naturally in sequencing from random fragments that just happen to have\nthe same breakpoints. However, as explained a bit more on the ",(0,i.kt)("a",{parentName:"p",href:"/whg-training-resources/next_generation_sequencing/Introduction%20to%20next-generation%20sequencing%20data%20analysis/Short_read_theory"},"paired-end sequencing theory"),"\npage, they also arise artifically from amplification and other\nchemistry artifacts, so it makes sense to assess levels of duplication in the data. That's what\nthis plot does. Here, it says that most reads are not duplicated, but a few appear twice or more."),(0,i.kt)("p",null,"Does that look sensible? Well, if we sequence a genome to a high level of coverage we'll of course\nget some duplicates just by chance. To assess that, let's ignore the paired end-y-ness for a moment\nand imagine sequencing the malaria genome with 2 million single-end 100 base pair reads. We could\nsimulate that by just choosing random read start points - here's a bit of R code that does that:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"# make sure to run this in an R session - not in the terminal!\n\nsimulate_duplicates = function( genome_size, number_of_reads ) {\n  # Sample read start locations\n  read_locations = sample( 1:genome_size, size = number_of_reads, replace = T )\n  # tabulate where the reads hit and make a histogram.\n  reads_per_location = table( read_locations )\n  M = max( max( reads_per_location ), 10 )\n  # Make a histogram\n  h = hist(\n    reads_per_location,\n    breaks = 0.5 + 0:M,\n    plot = FALSE\n  )\n  # Count duplicates (one per each additional copy)\n  duplicates = sum( 1:(M-1) * h$counts[2:M] )\n  return( list(\n    duplicate_rate = duplicates / number_of_reads,\n    histogram = h\n  ))\n}\n\nd = simulate_duplicates( 23E6, 2E6 )\n")),(0,i.kt)("p",null,"This returns a duplication rate of about 4% - let's plot it:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"plot(\n  1:10,\n  d$histogram$density * 100,\n  xlab = \"Sequence duplication level\",\n  ylab = \"Proportion (%)\",\n  type = 'l',\n  ylim = c(0, 100),\n  bty = 'n'\n)\n")),(0,i.kt)("p",null,"This shows:"),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"img",src:t(5900).Z,width:"400",height:"300"})),(0,i.kt)("p",null,"So we ought to get some duplicates anyway - even if everything else was perfect. (In reality,\nsequence coverage is far from uniform so we could expect a higher number of duplicates; on the\nother hand sequencing errors can make detection less sensitive.)"),(0,i.kt)("p",null,"Second, fastqc ",(0,i.kt)("strong",{parentName:"p"},"does not take into account the read pairs here")," - it analyses each fastq file\nseperately. Artifically-generated duplicates generally duplicate the whole fragment, meaning that\n",(0,i.kt)("strong",{parentName:"p"},"both read 1 and read 2 should be duplicated."),"  So fastqc might ",(0,i.kt)("em",{parentName:"p"},"overestimate")," the duplication rate."),(0,i.kt)("p",null,"Third, genomes contain duplicated sequence anyway, so some level of duplication is expected."),(0,i.kt)("p",null,"My hunch is that the 3% duplication rate output by fastqc above is an underestimate. You could try\ncomparing the ",(0,i.kt)("a",{parentName:"p",href:"/whg-training-resources/next_generation_sequencing/Introduction%20to%20next-generation%20sequencing%20data%20analysis/Aligning_reads"},"number of reads that are marked as duplicates")," in the alignment section."))}g.isMDXComponent=!0},5900:function(e,n,t){n.Z=t.p+"assets/images/simulated_duplicates-416e2cfb1869287143412f9314297a9b.jpg"}}]);