"use strict";(self.webpackChunkwhg_training_resources=self.webpackChunkwhg_training_resources||[]).push([[6109],{3905:function(e,t,n){n.d(t,{Zo:function(){return p},kt:function(){return m}});var a=n(7294);function i(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function o(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function r(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?o(Object(n),!0).forEach((function(t){i(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):o(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function l(e,t){if(null==e)return{};var n,a,i=function(e,t){if(null==e)return{};var n,a,i={},o=Object.keys(e);for(a=0;a<o.length;a++)n=o[a],t.indexOf(n)>=0||(i[n]=e[n]);return i}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(a=0;a<o.length;a++)n=o[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(i[n]=e[n])}return i}var s=a.createContext({}),u=function(e){var t=a.useContext(s),n=t;return e&&(n="function"==typeof e?e(t):r(r({},t),e)),n},p=function(e){var t=u(e.components);return a.createElement(s.Provider,{value:t},e.children)},h={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},c=a.forwardRef((function(e,t){var n=e.components,i=e.mdxType,o=e.originalType,s=e.parentName,p=l(e,["components","mdxType","originalType","parentName"]),c=u(n),m=i,d=c["".concat(s,".").concat(m)]||c[m]||h[m]||o;return n?a.createElement(d,r(r({ref:t},p),{},{components:n})):a.createElement(d,r({ref:t},p))}));function m(e,t){var n=arguments,i=t&&t.mdxType;if("string"==typeof e||i){var o=n.length,r=new Array(o);r[0]=c;var l={};for(var s in t)hasOwnProperty.call(t,s)&&(l[s]=t[s]);l.originalType=e,l.mdxType="string"==typeof e?e:i,r[1]=l;for(var u=2;u<o;u++)r[u]=n[u];return a.createElement.apply(null,r)}return a.createElement.apply(null,n)}c.displayName="MDXCreateElement"},7691:function(e,t,n){n.r(t),n.d(t,{assets:function(){return p},contentTitle:function(){return s},default:function(){return m},frontMatter:function(){return l},metadata:function(){return u},toc:function(){return h}});var a=n(7462),i=n(3366),o=(n(7294),n(3905)),r=["components"],l={},s=void 0,u={unversionedId:"statistical_modelling/Introduction/practicals/warmup/estimating an allele frequency",id:"statistical_modelling/Introduction/practicals/warmup/estimating an allele frequency",title:"estimating an allele frequency",description:"The frequency of O blood group",source:"@site/docs/statistical_modelling/Introduction/practicals/warmup/estimating an allele frequency.md",sourceDirName:"statistical_modelling/Introduction/practicals/warmup",slug:"/statistical_modelling/Introduction/practicals/warmup/estimating an allele frequency",permalink:"/whg-training-resources/statistical_modelling/Introduction/practicals/warmup/estimating an allele frequency",draft:!1,editUrl:"https://github.com/whg-training/whg-training-resources/docs/statistical_modelling/Introduction/practicals/warmup/estimating an allele frequency.md",tags:[],version:"current",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"distributions warmup",permalink:"/whg-training-resources/statistical_modelling/Introduction/practicals/warmup/distributions warmup"},next:{title:"probability warmup",permalink:"/whg-training-resources/statistical_modelling/Introduction/practicals/warmup/probability warmup"}},p={},h=[{value:"The frequency of O blood group",id:"the-frequency-of-o-blood-group",level:2},{value:"An aside on the ABO blood group system.",id:"an-aside-on-the-abo-blood-group-system",level:3},{value:"Loading the data",id:"loading-the-data",level:3},{value:"Building a model",id:"building-a-model",level:3},{value:"Plotting the unnormalised posterior",id:"plotting-the-unnormalised-posterior",level:3},{value:"Plotting the (fully normalised) posterior",id:"plotting-the-fully-normalised-posterior",level:3},{value:"A Beta way to do it",id:"a-beta-way-to-do-it",level:3},{value:"Comparing estimates",id:"comparing-estimates",level:3},{value:"Easily summarising our inference about the parameter.",id:"easily-summarising-our-inference-about-the-parameter",level:4},{value:"Dealing with very uncertain estimates",id:"dealing-with-very-uncertain-estimates",level:4}],c={toc:h};function m(e){var t=e.components,l=(0,i.Z)(e,r);return(0,o.kt)("wrapper",(0,a.Z)({},c,l,{components:t,mdxType:"MDXLayout"}),(0,o.kt)("h2",{id:"the-frequency-of-o-blood-group"},"The frequency of O blood group"),(0,o.kt)("p",null,"In this section we will estimate the frequency of O blood group (as encoded by the ",(0,o.kt)("a",{parentName:"p",href:"https://www.ensembl.org/Homo_sapiens/Variation/Explore?v=rs8176719"},"common\nloss-of-function deletion in the ABO\ngene"),", in multiple populations."),(0,o.kt)("h3",{id:"an-aside-on-the-abo-blood-group-system"},"An aside on the ABO blood group system."),(0,o.kt)("p",null,"The ABO blood group system was the first genetic polymorphism discovered in humans - ",(0,o.kt)("a",{parentName:"p",href:"https://www.ncbi.nlm.nih.gov/books/NBK2267/"},"long before\nthe structure of DNA was solved"),". It was discovered by\nstudying agglutination patterns of red cells in serum from other individuals and is of course of\nextreme relevance to blood transfusion. Beyond this, however, ",(0,o.kt)("em",{parentName:"p"},"ABO")," is an interesting gene; the A/B\nsplit seems to have been preserved ",(0,o.kt)("a",{parentName:"p",href:"https://www.pnas.org/content/109/45/18493"},"under balancing\nselection")," across primates, while the O mutation itself\nmay be a recurrent polymorphism. ABO is also ",(0,o.kt)("a",{parentName:"p",href:"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5207801/"},"one of the most pleiotropic loci in the human\ngenome")," and there is evidence it is\n",(0,o.kt)("a",{parentName:"p",href:"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7594382/"},"associated with risk of SARS-CoV-2\ninfection"),". "),(0,o.kt)("p",null,"But I was interested because it is also ",(0,o.kt)("a",{parentName:"p",href:"https://www.nature.com/articles/s41467-019-13480-z"},"protective against\nmalaria"),". (The data here comes from that paper)."),(0,o.kt)("h3",{id:"loading-the-data"},"Loading the data"),(0,o.kt)("p",null,"First, let's load the data."),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},"Note.")," I am using the ",(0,o.kt)("a",{parentName:"p",href:"https://www.tidyverse.org"},"tidyverse")," for these examples. If you don't\nhave it installed, either install it or use base R (e.g. ",(0,o.kt)("inlineCode",{parentName:"p"},"read.csv()"),") instead."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},'library( tidyverse )\ndata = read_csv( "o_bld_grp.csv" )\n')),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},"Question"),".  What countries are in the data?  What ethnic groups?"),(0,o.kt)("p",null,"The O blood group data is in the column called ",(0,o.kt)("inlineCode",{parentName:"p"},"O.bld.grp"),". A ",(0,o.kt)("inlineCode",{parentName:"p"},"1")," in this column means the\nindividual has O blood group (which happens if they have two copies of the above rs8176719\ndeletion). A ",(0,o.kt)("inlineCode",{parentName:"p"},"0")," generally means they will have either A, AB, or B blood group depending on the\nalleles they carry. (There are actually a few other mutations that cause loss of function of the ",(0,o.kt)("em",{parentName:"p"},"ABO")," gene,\nbut we're ignoring them here.)"),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},"Question"),". Table this data for each population or ethnicity. Can you estimate the frequency in\neach population? In each ethnic group?"),(0,o.kt)("h3",{id:"building-a-model"},"Building a model"),(0,o.kt)("p",null,"You probably estimated the allele frequency as: the number of O individuals divided by the total\nnumber of individuals.  If so congratulations - that's a sensible estimate!"),(0,o.kt)("p",null,"But hang on... some of them vary quite a bit, don't they?"),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},"Question.")," How much do you trust these estimates? Do you trust them all as much as each other?  Why?"),(0,o.kt)("p",null,"This course is all about handling uncertainty. We want to know ",(0,o.kt)("strong",{parentName:"p"},"both")," a good estimate, and how\nuncertain that estimate is.  This is what statistical models are for!"),(0,o.kt)("p",null,"Let's focus on a single population first - say Tanzania:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},'w = which( data$country == "Tanzania" )\n> table( data$country[w], data$O.bld.grp[w] )\n          \n            0  1\n  Tanzania 51 47\n')),(0,o.kt)("p",null,"Counts like this can be modelled well using a ",(0,o.kt)("a",{target:"_blank",href:n(4702).Z},"binomial distribution"),".  Let's do that now."),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},"Question"),". If we use a binomial distribution to model these counts, what assumptions are we making?"),(0,o.kt)("p",null,"The binomial distribution takes two parameters: ",(0,o.kt)("inlineCode",{parentName:"p"},"n"),", the number of 'trials' (i.e. samples), and ",(0,o.kt)("em",{parentName:"p"},"\u03b8"),", the\nfrequency. (","\u03b8"," is called ",(0,o.kt)("em",{parentName:"p"},"p")," on the ",(0,o.kt)("a",{target:"_blank",href:n(5479).Z},"probability%20cheatsheet"),", but we'll\ncall it ","\u03b8"," here.)"),(0,o.kt)("p",null,"The data is ",(0,o.kt)("em",{parentName:"p"},"k")," - the number of O blood group alleles observed. Our conceptual model is that the population 'emits' O\nblood group alleles at frequency ","\u03b8",". It is this frequency that we want to infer."),(0,o.kt)("p",null,"The basic inference formula (Bayes rule) is:"),(0,o.kt)("p",null,"$$ P(\\theta=x|\\text{data}) = \\frac{P(\\text{data}|\\theta=x) \\cdot P(\\theta=x)}{P(\\text{data})} $$"),(0,o.kt)("p",null,"For the moment let us ignore the prior term ",(0,o.kt)("em",{parentName:"p"},"P(","\u03b8","=x)"),". Or to put it another way, we will assume it is uniform."),(0,o.kt)("p",null,"The term P(data|","\u03b8","=x) is our ",(0,o.kt)("em",{parentName:"p"},"likelihood function"),". This is what we will model with a binomial distribution. That\nis, for the Tanzania data above we will assume:"),(0,o.kt)("p",null,"$$ P\\left(\\text{data}|\\theta=x\\right) = \\text{binom}\\left( k=47 | n=98, \\theta=x \\right) $$"),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},"Note.")," As in the ",(0,o.kt)("a",{target:"_blank",href:n(5479).Z},"probability cheatsheet")," you must remember that all\nprobability is conditional. In all probabilities above we are assuming this ",(0,o.kt)("em",{parentName:"p"},"highly unrealistic")," but ",(0,o.kt)("em",{parentName:"p"},"hopefully useful"),"\nmodel in which a single frequency parameter governs O blood group in an entire human population. The probabilities\ndon't exist in the real world, but only in the model."),(0,o.kt)("h3",{id:"plotting-the-unnormalised-posterior"},"Plotting the unnormalised posterior"),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},"Question.")," Given the counts for Tanzania above, plot the unnormalised posterior of the parameter ","\u03b8",". Also add\non a line showing your point estimate."),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},"Note.")," Because we are using a flat prior and the normalising constant here, this is the same as plotting the\nlikelihood function - the only R function you really need for this is ",(0,o.kt)("inlineCode",{parentName:"p"},"dbinom()"),". For readability it can be nice to\nmake a friendlier function matching the notation above:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"binomial.likelihood <- function( k, n, theta ) {\n    return( dbinom( x = k, size = n, prob = theta ) )\n}\n")),(0,o.kt)("p",null,"When you plot this you should see something like this:"),(0,o.kt)("p",null,(0,o.kt)("img",{alt:"img",src:n(5904).Z,width:"384",height:"192"}),"\n",(0,o.kt)("strong",{parentName:"p"},"Note.")," You did label your axes, right?  Hey, you must always label your axes!"),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},"Question.")," Can you make a grid of these plots, one per population?  One per ethnicity?"),(0,o.kt)("h3",{id:"plotting-the-fully-normalised-posterior"},"Plotting the (fully normalised) posterior"),(0,o.kt)("p",null,"In principle computing the posterior is not hard. We just need to compute the denominator of Bayes\ntheorem above, which (using the ",(0,o.kt)("em",{parentName:"p"},"law of total probability")," from the ",(0,o.kt)("a",{target:"_blank",href:n(5479).Z},"probability cheatsheet"),") is:"),(0,o.kt)("p",null,"$$ P(\\text{data}) = \\int_y P(\\text{data}|\\theta=y) P(\\theta=y) $$"),(0,o.kt)("p",null,"You could for example numerically compute this using the ",(0,o.kt)("inlineCode",{parentName:"p"},"integrate()")," function - e.g. using the Tanzania counts above:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"f <- function( y ) { return( binomial.likelihood( 47, 98, y ) ) ; }\ndenominator = integrate( f, 0, 1 )$value\n")),(0,o.kt)("p",null,"It we now plot the (normalised) posterior, it looks the same as the likelihood but the y axis scale is different:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"plot_normalised_posterior <- function( k, n ) {\n    f <- function( y ) { return( binomial.likelihood( k, n, y ) ) ; }\n    denominator = integrate( f, 0, 1 )$value\n\n    x = seq( from = 0, to = 1, by = 0.01 )\n    plot(\n        x, binomial.likelihood( k = k, n = n, theta = x ) / denominator,\n        type = 'l',\n        xlab = \"O blood group frequency (\u03b8)\",\n        ylab = \"Normalised posterior\",\n        yaxt = 'n',\n        bty = 'n'\n    )\n    grid()\n    abline( v = k/n, col = 'red' )\n    axis( 2, las = 1 )\n}\nplot_normalised_posterior( 47, 98 )\n")),(0,o.kt)("p",null,(0,o.kt)("img",{alt:"img",src:n(4029).Z,width:"384",height:"192"}),"\n(If you stare at this a bit you'll see it looks about right in terms of the total mass under the function - which\nshould sum to 1)."),(0,o.kt)("h3",{id:"a-beta-way-to-do-it"},"A Beta way to do it"),(0,o.kt)("p",null,"In the general case solving the denominator can indeed be hard and require numerical integration like the above.\nHowever, for this binomial model it turns out to be much easier. This is because the binomial has a 'conjugate' family of prior\ndistributions - the ",(0,o.kt)("a",{target:"_blank",href:n(4702).Z},"Beta distribution"),"."),(0,o.kt)("p",null,"If you stare at the Beta and binomial distributions ",(0,o.kt)("a",{target:"_blank",href:n(4702).Z},"on the cheatsheet"),"\nyou'll see why this works. Using our notation here, the parameter ","\u03b8"," (called ",(0,o.kt)("em",{parentName:"p"},"p")," on the cheatsheet) enters the\nbinomial distribution pdf in the term:"),(0,o.kt)("p",null,"$$ \\theta^k (1-\\theta)^n-k $$"),(0,o.kt)("p",null,"on the other hand, the frequency parameter of the Beta distribution (called ",(0,o.kt)("em",{parentName:"p"},"x"),") on the cheatsheet) enters its pdf in the\nsimilar term:"),(0,o.kt)("p",null,"$$ \\theta^{\\alpha-1} (1-\\theta)^{\\beta-1} $$"),(0,o.kt)("p",null,"All the other terms are constant as far as the frequency parameter is concerned. What all this means is that ",(0,o.kt)("strong",{parentName:"p"},"for a uniform (or\nBeta) prior, and a binomial likelihood, the posterior distribution is a Beta distribution"),":"),(0,o.kt)("p",null,"$$ \\text{Beta prior} \\rightarrow \\text{Binomial likelihood} \\Rightarrow \\text{Beta posterior} $$"),(0,o.kt)("p",null,"This relationship between prior and likelihood is known as 'conjugacy' - we say the Beta prior is conjugate to the Binomial\nlikelihood."),(0,o.kt)("p",null,"We can prove this for our data by adding it to our plot:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"x = seq( from = 0, to = 1, by = 0.01 )\npoints( x, dbeta( x, shape1 = 47+1, shape2 = 51+1 ), type = 'l', lty = 2, col = 'blue' )\n")),(0,o.kt)("p",null,(0,o.kt)("img",{alt:"img",src:n(6480).Z,width:"432",height:"192"}),"\n",(0,o.kt)("strong",{parentName:"p"},"Note.")," The ",(0,o.kt)("inlineCode",{parentName:"p"},"+1s")," are there in the above because of the way Beta is defined.  (It has exponents with ",(0,o.kt)("inlineCode",{parentName:"p"},"-1s"),".)"),(0,o.kt)("h3",{id:"comparing-estimates"},"Comparing estimates"),(0,o.kt)("p",null,"Having this analytical version of the posterior is good news. Here are two ways it helps. "),(0,o.kt)("h4",{id:"easily-summarising-our-inference-about-the-parameter"},"Easily summarising our inference about the parameter."),(0,o.kt)("p",null,"For example, we could summarise our inference by computing a ",(0,o.kt)("strong",{parentName:"p"},"credible interval")," for our parameter. A good interval\nto take is a 95% posterior mass interavls, which you can form by chopping of 2.5% of the mass from both tails of the\ndistribution. If you want to see how to do this, look at the diagram ",(0,o.kt)("a",{target:"_blank",href:n(4702).Z},"on the probability cheatsheet"),". In R, the beta cdf is given by ",(0,o.kt)("inlineCode",{parentName:"p"},"pbeta()"),", and its inverse (the\nquantile function) by ",(0,o.kt)("inlineCode",{parentName:"p"},"qbeta()"),". So we can do something like this:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"compute.credible.interval <- function( k, n, mass = 0.95 ) {\n    tail = 1-mass\n    return( qbeta(\n        c(\n            lower = tail/2,\n            median = 0.5,\n            upper = 1.0-(tail/2)\n        ),\n        shape1 = k+1,\n        shape2 = n-k+1\n    ))\n}\n")),(0,o.kt)("p",null,"And let's use it to make a function to easily summarise any given subset of the data:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"compute.counts <- function( data ) {\n    return( c(\n        nonO = length( which( data$O.bld.grp == 0 )),\n        O = length( which( data$O.bld.grp == 1 ))\n    ))\n}\nsummarise <- function( data.subset, name ) {\n    counts = compute.counts( data.subset )\n    credible = compute.credible.interval( counts[2], sum(counts ))\n    return( c(\n        list( name = name, nonO = counts[1], O = counts[2], estimate = counts[2]/sum(counts) ),\n        credible\n    ))\n}\n")),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},"Question")," Compute the 95% credible interval for all the populations. Compare to the sample size. (For example - can\nyou plot sample size vs size of credible interval?). What is the pattern?"),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},"Question.")," Do all the 95% credible intervals overlap? What about for individual ethnicities?"),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},"Hint.")," A neat way to do this is to use ",(0,o.kt)("inlineCode",{parentName:"p"},"map_dfr()")," from the ",(0,o.kt)("a",{parentName:"p",href:"https://github.com/rstudio/cheatsheets/blob/main/purrr.pdf"},"purr package"),":"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},'countries = unique( data$country )\nmap_dfr( countries, function(country) { summarise( data[ data$country == country, ], country ) })\n\ndata$country_ethnicity = sprintf( "%s:%s", data$country, data$ethnicity )\nethnicities = unique( data$country_ethnicity )\nmap_dfr( ethnicities, function(ethnicity) { summarise( data[ data$country_ethnicity == ethnicity, ], ethnicity ) })\n')),(0,o.kt)("p",null,"We could also add these to our plots - see the code in ",(0,o.kt)("a",{target:"_blank",href:n(4955).Z},(0,o.kt)("code",null,"solutions/solutions.R"))," for a working\nversion:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},'source( "solutions/solutions.R")\nplot.bygroup( data, "country_ethnicity" )\n')),(0,o.kt)("p",null,(0,o.kt)("img",{alt:"img",src:n(7223).Z,width:"768",height:"768"})),(0,o.kt)("h4",{id:"dealing-with-very-uncertain-estimates"},"Dealing with very uncertain estimates"),(0,o.kt)("p",null,"Another way the conjugate prior / analytical formulation helps is in dealing with very uncertain estimates. For\nexample, the estimate for the ",(0,o.kt)("inlineCode",{parentName:"p"},"AKANS[ASHANTI_EASTERN]")," has a point estimate of > 0.6 even though most of the others are\n< 0.8. But this is based on only 21 observations. And what about the ",(0,o.kt)("inlineCode",{parentName:"p"},"Ghana:NORTHENER")," or other groups where the\nestimate is one or zero, but the counts are tiny?"),(0,o.kt)("p",null,"One way we can deal with this is give up on the point estimate - just satisfy ourselves that there isn't much evidence.\nFor example, across all samples the joint estimate is ",(0,o.kt)("inlineCode",{parentName:"p"},"0.425"),". How likely is it that the ",(0,o.kt)("inlineCode",{parentName:"p"},"AKANS[ASHANTI_EASTERN]"),"\nestimate is larger than this?"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"> 1 - pbeta( 0.425, shape1 = 9, shape2 = 13 ) \n[1] 0.4301758\n")),(0,o.kt)("p",null,"There's only 43% chance - that's a 57% chance the estimate is at or lower than the joint estimate.  "),(0,o.kt)("p",null,"However, it can nevertheless be useful in lots of contexts to get a sensible point estimate, even when there is little\ndata. To do this let's replace our uniform prior with a ",(0,o.kt)("em",{parentName:"p"},"weakly informative prior"),". This prior will say that we believe\nO blood group is not at 0 or 100% frequency. If you followed the above, you'll see that we can just replace our uniform\nprior with a ",(0,o.kt)("inlineCode",{parentName:"p"},"Beta(2,2)")," prior.  Equivalently, we add 1 to boht the non-O and O counts in the data.  Try this:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},'plot.bygroup( data, "country_ethnicity", prior.counts = c(1,1) )\n')),(0,o.kt)("p",null,"Now none of the estimates are 1 or zero, and all the credible intervals look at least somewhat sensible. Our prior has\n",(0,o.kt)("strong",{parentName:"p"},"regularised")," the estimation."),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},"Question"),". Try adjusting the prior counts and seeing what happens to the estimates. Does it affect all populations\nequally? How much prior data do you need to get all the estimates within 0.1 of each other?"),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},"Note.")," This technique is often used in practice. For example, the famous ",(0,o.kt)("a",{parentName:"p",href:"https://www.nature.com/articles/ng1847"},"EIGENSTRAT\nsoftware")," for computing principal components analysis estimates\nallele frequencies in just this way - with a Beta(2,2) prior."))}m.isMDXComponent=!0},4702:function(e,t,n){t.Z=n.p+"assets/files/Distributions cheatsheet-cc4e21d39d4e4e0af58815c6c5bc670b.pdf"},5479:function(e,t,n){t.Z=n.p+"assets/files/Probability cheatsheet-2344e889eac5732144ee5396d655e561.pdf"},4955:function(e,t,n){t.Z=n.p+"assets/files/solutions-e956342871da265a3e7f29cd31b729df.R"},5904:function(e,t,n){t.Z=n.p+"assets/images/Tanzania_o_blood_group_likelihood-76977c5b769a3216541fb08227562812.svg"},6480:function(e,t,n){t.Z=n.p+"assets/images/Tanzania_o_blood_group_posterior+beta-64925d62c2588a674962821752fa0847.svg"},4029:function(e,t,n){t.Z=n.p+"assets/images/Tanzania_o_blood_group_posterior-5882b1f9c8de9b7da4e86dcdff7b748e.svg"},7223:function(e,t,n){t.Z=n.p+"assets/images/all_ethnicities_o_blood_group_beta_posterior-8f6e1249c4499b4c5fa8f4fff7fa0cda.svg"}}]);