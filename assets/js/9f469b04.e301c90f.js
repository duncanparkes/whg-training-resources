"use strict";(self.webpackChunkwhg_training_resources=self.webpackChunkwhg_training_resources||[]).push([[650],{3905:function(t,e,a){a.d(e,{Zo:function(){return c},kt:function(){return d}});var n=a(7294);function o(t,e,a){return e in t?Object.defineProperty(t,e,{value:a,enumerable:!0,configurable:!0,writable:!0}):t[e]=a,t}function i(t,e){var a=Object.keys(t);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(t);e&&(n=n.filter((function(e){return Object.getOwnPropertyDescriptor(t,e).enumerable}))),a.push.apply(a,n)}return a}function r(t){for(var e=1;e<arguments.length;e++){var a=null!=arguments[e]?arguments[e]:{};e%2?i(Object(a),!0).forEach((function(e){o(t,e,a[e])})):Object.getOwnPropertyDescriptors?Object.defineProperties(t,Object.getOwnPropertyDescriptors(a)):i(Object(a)).forEach((function(e){Object.defineProperty(t,e,Object.getOwnPropertyDescriptor(a,e))}))}return t}function s(t,e){if(null==t)return{};var a,n,o=function(t,e){if(null==t)return{};var a,n,o={},i=Object.keys(t);for(n=0;n<i.length;n++)a=i[n],e.indexOf(a)>=0||(o[a]=t[a]);return o}(t,e);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(t);for(n=0;n<i.length;n++)a=i[n],e.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(t,a)&&(o[a]=t[a])}return o}var l=n.createContext({}),p=function(t){var e=n.useContext(l),a=e;return t&&(a="function"==typeof t?t(e):r(r({},e),t)),a},c=function(t){var e=p(t.components);return n.createElement(l.Provider,{value:e},t.children)},u={inlineCode:"code",wrapper:function(t){var e=t.children;return n.createElement(n.Fragment,{},e)}},h=n.forwardRef((function(t,e){var a=t.components,o=t.mdxType,i=t.originalType,l=t.parentName,c=s(t,["components","mdxType","originalType","parentName"]),h=p(a),d=o,m=h["".concat(l,".").concat(d)]||h[d]||u[d]||i;return a?n.createElement(m,r(r({ref:e},c),{},{components:a})):n.createElement(m,r({ref:e},c))}));function d(t,e){var a=arguments,o=e&&e.mdxType;if("string"==typeof t||o){var i=a.length,r=new Array(i);r[0]=h;var s={};for(var l in e)hasOwnProperty.call(e,l)&&(s[l]=e[l]);s.originalType=t,s.mdxType="string"==typeof t?t:o,r[1]=s;for(var p=2;p<i;p++)r[p]=a[p];return n.createElement.apply(null,r)}return n.createElement.apply(null,a)}h.displayName="MDXCreateElement"},9023:function(t,e,a){a.r(e),a.d(e,{assets:function(){return c},contentTitle:function(){return l},default:function(){return d},frontMatter:function(){return s},metadata:function(){return p},toc:function(){return u}});var n=a(7462),o=a(3366),i=(a(7294),a(3905)),r=["components"],s={},l="A tale of two 2x2 tables",p={unversionedId:"statistical_modelling/Introduction/practicals/a_tale_of_two_2x2_tables/introduction",id:"statistical_modelling/Introduction/practicals/a_tale_of_two_2x2_tables/introduction",title:"A tale of two 2x2 tables",description:"Welcome to (possibly) the world's first choose your own statistical adventure!",source:"@site/docs/statistical_modelling/Introduction/practicals/a_tale_of_two_2x2_tables/introduction.md",sourceDirName:"statistical_modelling/Introduction/practicals/a_tale_of_two_2x2_tables",slug:"/statistical_modelling/Introduction/practicals/a_tale_of_two_2x2_tables/introduction",permalink:"/whg-training-resources/statistical_modelling/Introduction/practicals/a_tale_of_two_2x2_tables/introduction",draft:!1,editUrl:"https://github.com/whg-training/whg-training-resources/docs/statistical_modelling/Introduction/practicals/a_tale_of_two_2x2_tables/introduction.md",tags:[],version:"current",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"When asymptotics go wrong",permalink:"/whg-training-resources/statistical_modelling/Introduction/practicals/a_tale_of_two_2x2_tables/When asymptotics go wrong"},next:{title:"Bayesian meta-analysis practical",permalink:"/whg-training-resources/statistical_modelling/Introduction/practicals/bayesian_meta_analysis/bayesian_meta_analysis_practical"}},c={},u=[{value:"Modelling 2x2 tables",id:"modelling-2x2-tables",level:2},{value:"When asymptotics go wrong",id:"when-asymptotics-go-wrong",level:2}],h={toc:u};function d(t){var e=t.components,s=(0,o.Z)(t,r);return(0,i.kt)("wrapper",(0,n.Z)({},h,s,{components:e,mdxType:"MDXLayout"}),(0,i.kt)("h1",{id:"a-tale-of-two-2x2-tables"},"A tale of two 2x2 tables"),(0,i.kt)("p",null,"Welcome to (possibly) the world's first ",(0,i.kt)("em",{parentName:"p"},"choose your own statistical adventure"),"!"),(0,i.kt)("p",null,"In this practical we are going to study the following two two-by-two tables, which come from two\npublished papers about genetic variants that affect malaria susceptibility:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-R"},"                       non-O      O                                          rs60822373   rs60822373\n                       blood gp.  blood gp.                                  G allele     C allele\n            controls:  3420       3233               unexposed populations:  1965         1\nsevere malaria cases:  3925       2738         malaria-exposed populations:  707          17\n")),(0,i.kt)("p",null,"The left table contains data from ",(0,i.kt)("a",{parentName:"p",href:"https://doi.org/10.1038/s41467-019-13480-z"},"https://doi.org/10.1038/s41467-019-13480-z"),", and shows that O blood group is at\nlower frequency in severe malaria cases than in the general population, consistent with a protective effect of O blood\ngroup."),(0,i.kt)("p",null,"The right table comes from ",(0,i.kt)("a",{parentName:"p",href:"https://science.sciencemag.org/content/348/6235/711"},"https://science.sciencemag.org/content/348/6235/711"),", and shows that the rs60822373 'C'\nalleles is at higher frequency in malaria-exposed population (here sub-Saharan Africans) than in non-exposed\npopulations (European-ancestry individuals). rs60822373 encodes the Cromer blood group, so this is consistent with a\nprotective effect of the Cromer blood group on malaria. (In this right table, the data actually comes from the ",(0,i.kt)("a",{parentName:"p",href:"http://www.nature.com/nature/journal/v491/n7422/full/nature11632.html"},"1000\nGenomes Project phase I"),")."),(0,i.kt)("p",null,"If you run a statistical test on either table you will see that these are highly statistically significant differences\nin frequencies between the two rows of each table, so these differences are not just due to sampling."),(0,i.kt)("p",null,"But ",(0,i.kt)("em",{parentName:"p"},"something is wrong with one of these tables"),".  Your mission is to find out what!"),(0,i.kt)("p",null,(0,i.kt)("em",{parentName:"p"},"QN"),".  Maybe you can already see what is wrong?  Give it some thought before we proceed."),(0,i.kt)("h2",{id:"modelling-2x2-tables"},"Modelling 2x2 tables"),(0,i.kt)("p",null,"To quantify the effects in both tables we need a model. In both tables, the data was collected based on the row labels\n(i.e. case/control or population of collection) so the most straightforward model is of ",(0,i.kt)("em",{parentName:"p"},"binomial sampling in rows"),".\nIn other words, for a table:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"\n           X    Y        (total)\nunexposed  a    b        (N\u2081 = a+b)\n  exposed  c    d        (N\u2082 = c+d)\n")),(0,i.kt)("p",null,"We model the counts of ",(0,i.kt)("inlineCode",{parentName:"p"},"Y")," as:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"  b ~ binomial( n = N\u2081, p = \u03d1\u2081 )\n  d ~ binomial( n = N\u2082, p = \u03d1\u2082 )\n")),(0,i.kt)("p",null,"where \u03d1\u2081 and \u03d1\u2082 are frequencies in the two rows."),(0,i.kt)("p",null,(0,i.kt)("em",{parentName:"p"},"Exercise")," You can implement this model easily using the code we have already written - see ",(0,i.kt)("a",{target:"_blank",href:a(938).Z},"this file")," for an implementation."),(0,i.kt)("p",null,"However, we'd really like to parameterise the model so that it contains an ",(0,i.kt)("em",{parentName:"p"},"effect size parameter")," - a parameter that\nreflects how different the two row frequencies are.  We will use the ",(0,i.kt)("em",{parentName:"p"},"log odds ratio")," to do this.  In the above table, the observed log odds ratio is the quantity:"),(0,i.kt)("p",null,(0,i.kt)("img",{parentName:"p",src:"https://render.githubusercontent.com/render/math?math=%5Cdisplaystyle+%5Ctext%7Blog%7D+%5Ctext%7BOR%7D%3D%5Clog%5Cleft%28%5Cfrac%7Ba+d%7D%7Bb+c%7D%5Cright%29",alt:"\\text{log} \\text{OR}=\\log\\left(\\frac{a d}{b c}\\right)"})),(0,i.kt)("p",null,"There are a couple of reasons to focus on log odds ratio here.  One of them is mathematical convenience.  For a probability p, the odds is defined as p/(1-p).  The odds lies in ","[0,\u221e]"," and thus the log-odds lies in ","[-\u221e,\u221e]",".  It is generally useful to have parameters living in an unbounded space."),(0,i.kt)("p",null,"Thus, transforming to log odds allows us to put our parameters on the real line."),(0,i.kt)("p",null,"Another way to think of this is through the inverse function, which maps log odds back to probability space.  This function is known as the ",(0,i.kt)("em",{parentName:"p"},"logistic")," function and defined by:"),(0,i.kt)("p",null,(0,i.kt)("img",{parentName:"p",src:"https://render.githubusercontent.com/render/math?math=%5Cdisplaystyle+%5Ctext%7Blogistic%7D%28x%29+%3D+%5Cfrac%7Be%5Ex%7D%7B1%2Be%5Ex%7D",alt:"\\text{logistic}(x) = \\frac{e^x}{1+e^x}"})),(0,i.kt)("p",null,"Let's plot it:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-R"},"logistic <- function( x ) {\n    exp(x) / ( 1 + exp(x) )\n}\n\nx = seq( from = -10, to = 10, by = 0.01 )\nplot( x, logistic(x), type = 'l', bty = 'n' )\ngrid()\n")),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"logistic function",src:a(4470).Z,width:"379",height:"243"})),(0,i.kt)("p",null,"You can see that the logistic function maps the real line (log-odds space, x axis) to the unit interval (y axis, probability space).  It is smooth and tails off to being essentially flat outside around ","[-10,10]","."),(0,i.kt)("p",null,(0,i.kt)("em",{parentName:"p"},"Exercise")," prove that ",(0,i.kt)("inlineCode",{parentName:"p"},"logistic()")," is the inverse of the log odds (substitute one expression into the other and simplify). Alternatively, prove this to yourself by implementing both function in R."),(0,i.kt)("p",null,(0,i.kt)("em",{parentName:"p"},"Exercise")," (advanced) what is the slope of the logistic function at x=0?  (Hint: you need to compute the derivative.)"),(0,i.kt)("p",null,"Now that we are working in log-odds space (on the real line), this frees us up to express our log odds as a ",(0,i.kt)("em",{parentName:"p"},"baseline log odds")," (applying to both rows of the table) plus an ",(0,i.kt)("em",{parentName:"p"},"additional log odds conferred by the second row"),".  Let's call these parameters \u03bc and \u03b2.  We are thus writing:"),(0,i.kt)("p",null,(0,i.kt)("img",{parentName:"p",src:"https://render.githubusercontent.com/render/math?math=%5Cdisplaystyle+%5Ctheta_1+%3D+%5Ctext%7Blogistic%7D%28%5Cmu%29%0A%5Cquad%5Ctext%7Band%7D%5Cquad%0A%5Ctheta_2+%3D+%5Ctext%7Blogistic%7D%28%5Cmu%2B%5Cbeta%29%0A",alt:"\\theta_1 = \\text{logistic}(\\mu)\n\\quad\\text{and}\\quad\n\\theta_2 = \\text{logistic}(\\mu+\\beta)\n"})),(0,i.kt)("p",null,"Or equivalently:"),(0,i.kt)("p",null,(0,i.kt)("img",{parentName:"p",src:"https://render.githubusercontent.com/render/math?math=%5Cdisplaystyle+%5Ctext%7Blog%7D+%5Ctext%7Bodds%7D%28%5Ctheta_1%29%3D%5Cmu%0A%5Cquad%5Ctext%7Band%7D%5Cquad%0A%5Ctext%7Blog%7D+%5Ctext%7Bodds%7D%5Cleft%28%5Ctheta_2%5Cright%29+%3D+%5Cmu%2B%5Cbeta%0A",alt:"\\text{log} \\text{odds}(\\theta_1)=\\mu\n\\quad\\text{and}\\quad\n\\text{log} \\text{odds}\\left(\\theta_2\\right) = \\mu+\\beta\n"})),(0,i.kt)("p",null,"such that \u03b2 is the log odds ratio."),(0,i.kt)("p",null,"How does this look in practice?  Lets implement it:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-R"},"# Requires binomial.ll first!\ntable.ll <- function( data, params = list( theta = c( 0.5, 0.5 )) ) {\n    # For a 2x2 table we assume binomial sampling in rows\n    # So sum the lls over the rows\n    return (\n        binomial.ll(\n            x = data[1,2],\n            params = list(\n                n = rowSums( data )[1],\n                p = params$theta[1]\n            )\n        )\n        + binomial.ll(\n            data[2,2],\n            params = list(\n                n = sum( data[2,] ),\n                p = params$theta[2]\n            )\n        )\n    )\n}\n\nlogistic <- function( x ) {\n    exp(x) / ( 1 + exp(x) )\n}\n\nreparameterised.table.ll <- function( data, params = list( theta = 0.5, log.or = 0 )) {\n    # params[1] is log-odds of baseline frequency\n    # params[2] is log odds ratio\n    theta = c(\n        logistic( params$theta ),\n        logistic( params$theta + params$log.or )\n    )\n    return( table.ll( data, params = list( theta = theta )))\n}\n\n")),(0,i.kt)("p",null,"(These functions rely on ",(0,i.kt)("a",{parentName:"p",href:"../loglikelihoods/binomial.ll"},"binomial.ll"),")."),(0,i.kt)("p",null,"Let's load up the tables:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},'tables = list(\n    case_control = matrix(\n        c( 3420, 3233, 3925, 2738 ),\n        byrow = T, ncol = 2,\n        dimnames = list(\n            c( "non-O", "O" ),\n            c( "controls", "cases" )\n        )\n    ),\n    exposure = matrix(\n        c( 1965, 1, 707, 17 ),\n        byrow = T, ncol = 2,\n        dimnames = list(\n            c( "G", "C" ),\n            c( "unexposed", "exposed" )\n        )\n    )\n)\n')),(0,i.kt)("p",null,"And plot the likelihood (this uses ",(0,i.kt)("a",{parentName:"p",href:"../loglikelihoods/inspect.ll"},"inspect.ll"),").)"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"fit1 = inspect.ll( reparameterised.table.ll, data = tables[[1]], params = list( theta = 0.5, log.or = 0 ) )\nfit2 = inspect.ll( reparameterised.table.ll, data = tables[[2]], params = list( theta = 0.5, log.or = 0 ) )\n")),(0,i.kt)("p",null,"Look at both these plots.  For the first table:"),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"Table 1 likelihood",src:a(7258).Z,width:"800",height:"600"})),(0,i.kt)("p",null,"The plot for the first table is about as good as this plot could look.  It estimates a log odds-ratio of -0.30, which is an odds ratio of ~0.74.  Also, the Gaussian fit to the likelihood is about as good as it could possibly be.  We could get a reasonable interval as:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-R"},'   > sprintf(\n       "%.2f ( %.2f - %.2f)",\n       exp( fit1$mle$log.or ),\n       exp( fit1$mle$log.or - 1.96 * fit1$standard.errors[1] ), exp( fit1$mle$log.or + 1.96 * fit1$standard.errors[1] )\n   )\n   [1] "0.74 ( 0.70 - 0.77)"\n')),(0,i.kt)("p",null,"As discussed in class, you can interpret this in one of two ways. The bayesian interpretation is that our uncertainty\nin the true odds ratio, given this particular data table, is well expressed by a small interval around 0.74. The above\ninterval is a 95% bayesian ",(0,i.kt)("em",{parentName:"p"},"credible interval")," for the parameter - meaning an interval that contains 95% of the\nposterior mass. (In principle this interpretation depends on assuming a flat prior, so that the posterior is\nproportional to the likelihood. But because the data is so strong here, the prior won't affect this much as long as the\nprior is relatively spread out). The posterior is pretty much Gaussian, so the probability that the parameter was\nactually zero, or something above zero, can be computed from the quantiles of the Gaussian distribution:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-R"},"> pnorm( 0, mean = fit1$mle$log.or, sd = fit1$standard.errors[2], lower.tail = F )\n[1] 1.711726e-18\n")),(0,i.kt)("p",null,"meaning that (assuming the model holds) it's vanishingly unlikely we have got the sign of this effect wrong."),(0,i.kt)("p",null,(0,i.kt)("em",{parentName:"p"},"Note")," this is a bit daft because we shouldn't be allowed a 'flat prior' here (there's no function that integrates to 1 over the whole real line).  And in any case a 'flat prior' would place most of its mass outside any finite interval so this prior really claims that the effect is huge.  Adding a ",(0,i.kt)("em",{parentName:"p"},"proper prior")," here is not hard - "),(0,i.kt)("p",null,(0,i.kt)("em",{parentName:"p"},"Exercise")," add a proper prior here (e.g. Gaussian with mean 0 and standard deviation sigma) - how does this change the above?"),(0,i.kt)("p",null,"Alternatively, we discussed in lectures how this can be interpreted as a frequentist statement about the parameter. In\nthis setting we think of ",(0,i.kt)("em",{parentName:"p"},"replicates of the table from the same sampling process")," and interpret the above interval as a\n",(0,i.kt)("em",{parentName:"p"},"confidence interval"),". To state this specifically, it means:"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Imagine an infinite number of replicates of the data with some 'true' parameters"),(0,i.kt)("li",{parentName:"ul"},"Imagine for each such data table we used the above procedure to compute a confidence interval"),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("em",{parentName:"li"},"Then")," the true parameter would be in the computed interval 95% of the time.")),(0,i.kt)("p",null,"If this strikes you as a complicated interpretation - it is! My general advice is to interpret this interval as a\nstatement about our uncertainty in the parameter given the model - i.e. as a bayesian credible interval. In more\ngeneral situations with less informative data you should think carefully about what prior is appropriate. Where the\nfrequentist approach becomes useful is in ",(0,i.kt)("em",{parentName:"p"},"calibrating")," or ",(0,i.kt)("em",{parentName:"p"},"model checking")," our model against reality.  In this case it's useful to know that ",(0,i.kt)("em",{parentName:"p"},"if the true log-odds were zero"),", then the estimate would be distributed like"),(0,i.kt)("p",null,(0,i.kt)("img",{parentName:"p",src:"https://render.githubusercontent.com/render/math?math=%5Cdisplaystyle+%5Chat%7B%5Cbeta%7D%7EN%280%2C%5Ctext%7Bse%7D%5E2%29",alt:"\\hat{\\beta}~N(0,\\text{se}^2)"})),(0,i.kt)("p",null,"so that a one-tailed P-value can be computed from quantiles of this normal distribution:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-R"},"> pnorm( abs(fit1$mle$log.or), mean = 0, sd = fit1$standard.errors[2], lower.tail = F )\n[1] 1.711726e-18\n")),(0,i.kt)("p",null,(0,i.kt)("em",{parentName:"p"},"Exercise")," This number is of course exactly equal to the Bayesian probability of getting the sign wrong above - satisfy yourself why this is true."),(0,i.kt)("h2",{id:"when-asymptotics-go-wrong"},"When asymptotics go wrong"),(0,i.kt)("p",null,"On the other hand, look at the plot for the 2nd table:"),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"Table 2 likelihood",src:a(6788).Z,width:"800",height:"600"})),(0,i.kt)("p",null,"Clearly something goes wrong with the asymptotics for this table.  The Gaussian approximation is not great.  Focussing on the log-odds ratio parameter, the approximation puts ",(0,i.kt)("em",{parentName:"p"},"too little weight")," on small odds ratios, and it puts ",(0,i.kt)("em",{parentName:"p"},"too much weight")," on larger odds ratios.  If we again compute a P-value"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-R"},"> pnorm( abs(fit2$mle$log.or), mean = 0, sd = fit2$standard.errors[2], lower.tail = F )\n[1] 9.020974e-05\n")),(0,i.kt)("p",null,"it is still statistically significant - but we should be cautious because the gaussian approximation is not great."),(0,i.kt)("p",null,"Here are two ways to deal with this.  First, we could use ",(0,i.kt)("em",{parentName:"p"},"Fisher's exact test")," intead.  Fisher's exact test deals with this situation by additionally conditioning on the column sums of the table.  This is like saying we knew what the frequency of the variant was beforehand"))}d.isMDXComponent=!0},938:function(t,e,a){e.Z=a.p+"assets/files/table.ll-a3f4177f29d917faced6ec349ec06988.R"},4470:function(t,e,a){e.Z=a.p+"assets/images/logistic-bd5b92de76b0e2e0f9ef99fb2d86f201.png"},7258:function(t,e,a){e.Z=a.p+"assets/images/table_1_ll-72c889489d8f211c811b13d11a536dc8.png"},6788:function(t,e,a){e.Z=a.p+"assets/images/table_2_ll-e81c604db2012c4841dacdb6caee1644.png"}}]);